{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff03763",
   "metadata": {},
   "source": [
    "## INFSYS 722 - Iteration 4\n",
    "ID: 168395378\n",
    "<br/>\n",
    "Author: Paolo Gabriel Averia Delos Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634cefaa",
   "metadata": {},
   "source": [
    "### 01 - Business Understanding\n",
    "\n",
    "This section, we describe the business goals and objectives.\n",
    "\n",
    "Our goal is o study and analyze global water treatment and water consumption. From there, compare the Philippines versus top countries and identify where the Philippines can improve on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1fc1c1",
   "metadata": {},
   "source": [
    "### 02 - Data Understanding\n",
    "\n",
    "Loading the dataset to grasp what the data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad93bd",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e49d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark libraries\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "spark = SparkSession.builder.appName('pdel145_iter4').getOrCreate()\n",
    "\n",
    "# Strictly for .xlsx datatypes only!\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96e714",
   "metadata": {},
   "source": [
    "#### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f540721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshwater dataset\n",
    "freshwater_df = spark.read.csv('data/freshwater.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ac6ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SDG: integer (nullable = true)\n",
      " |-- SDG target: double (nullable = true)\n",
      " |-- SDG indicator: string (nullable = true)\n",
      " |-- Indicator Code: string (nullable = true)\n",
      " |-- Indicator name: string (nullable = true)\n",
      " |-- Geographical area code: string (nullable = true)\n",
      " |-- Geographical area name: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Value: double (nullable = true)\n",
      " |-- Time detail: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Footnote: string (nullable = true)\n",
      " |-- Type of data: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Age group: string (nullable = true)\n",
      " |-- Bounds: string (nullable = true)\n",
      " |-- Frequency: string (nullable = true)\n",
      " |-- Level/Status: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Type of reporting: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- SDG 6 Data portal level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for its datatypes\n",
    "freshwater_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ecdec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+--------------+--------------------+----------------------+----------------------+----+-----+-----------+--------------------+--------+--------------------+----------+---------+------+---------+------------+--------------------+-----------------+----+-----------------------+\n",
      "|SDG|SDG target|SDG indicator|Indicator Code|      Indicator name|Geographical area code|Geographical area name|Year|Value|Time detail|              Source|Footnote|        Type of data|     Units|Age group|Bounds|Frequency|Level/Status|            Location|Type of reporting| Sex|SDG 6 Data portal level|\n",
      "+---+----------+-------------+--------------+--------------------+----------------------+----------------------+----+-----+-----------+--------------------+--------+--------------------+----------+---------+------+---------+------------+--------------------+-----------------+----+-----------------------+\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|          Turkmenistan|2012|  3.9|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|                Uganda|2012|  3.0|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|                Uganda|2012|  3.0|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|               Ukraine|2012| 3.71|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|               Ukraine|2012| 3.71|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|       North Macedonia|2012| 2.07|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|       North Macedonia|2012| 2.07|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|                 Egypt|2012|17.49|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|                 Egypt|2012|17.49|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|  United Kingdom of...|2012| 9.96|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|  United Kingdom of...|2012|  9.7|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|       Channel Islands|2012| null|  2008-2012|                null|    null|       Not Available|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|  United Republic o...|2012| 1.32|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|  United Republic o...|2012| 1.32|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|  United States of ...|2012| 3.51|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|  United States of ...|2012| 3.51|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|          Burkina Faso|2012| 3.59|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|          Burkina Faso|2012| 3.59|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|               Uruguay|2012|  1.1|  2008-2012|Food and Agricult...|    null|      Estimated data|Percentage|     null|  null|     null|        null|Services (ISIC4 G...|           Global|null|   6.4.2 Level of wa...|\n",
      "|  6|       6.4|        6.4.2| ER_H2O_STRESS|Level of water st...|               Country|               Uruguay|2012|  1.1|  2008-2012|Food and Agricult...|    null|Global monitoring...|      null|     null|  null|     null|        null|                null|             null|null|   6.4.2 Level of wa...|\n",
      "+---+----------+-------------+--------------+--------------------+----------------------+----------------------+----+-----+-----------+--------------------+--------+--------------------+----------+---------+------+---------+------------+--------------------+-----------------+----+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing what the dataset contains\n",
    "freshwater_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ebd7118e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field Geographical area code: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m tot_popul_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/total_population.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Reading datasets that are of excel (.xlsx) format\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m safe_manage_drink_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/safely_managed_drinking.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m safe_manage_sanit_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/safely_managed_sanitation.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/session.py:673\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    670\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:340\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    339\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/session.py:700\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/session.py:512\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    509\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 512\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    514\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/session.py:439\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema from empty dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 439\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/types.py:1109\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1108\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m b\u001b[38;5;241m.\u001b[39mfields)\n\u001b[0;32m-> 1109\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [StructField(f\u001b[38;5;241m.\u001b[39mname, _merge_type(f\u001b[38;5;241m.\u001b[39mdataType, nfs\u001b[38;5;241m.\u001b[39mget(f\u001b[38;5;241m.\u001b[39mname, NullType()),\n\u001b[1;32m   1110\u001b[0m                                               name\u001b[38;5;241m=\u001b[39mnew_name(f\u001b[38;5;241m.\u001b[39mname)))\n\u001b[1;32m   1111\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields]\n\u001b[1;32m   1112\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/types.py:1109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1108\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m b\u001b[38;5;241m.\u001b[39mfields)\n\u001b[0;32m-> 1109\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [StructField(f\u001b[38;5;241m.\u001b[39mname, \u001b[43m_merge_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNullType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1111\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields]\n\u001b[1;32m   1112\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/types.py:1104\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(b):\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;66;03m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not merge type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(a), \u001b[38;5;28mtype\u001b[39m(b))))\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# same type\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n",
      "\u001b[0;31mTypeError\u001b[0m: field Geographical area code: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>"
     ]
    }
   ],
   "source": [
    "# Same goes for the rest of the datasets that are of file format .csv\n",
    "impr_sanit_df = spark.read.csv('data/improved_sanitation.csv', inferSchema=True, header=True)\n",
    "renewable_df = spark.read.csv('data/renewable.csv', inferSchema=True, header=True)\n",
    "tot_popul_df = spark.read.csv('data/total_population.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Reading datasets that are of excel (.xlsx) format\n",
    "safe_manage_drink_df = spark.createDataFrame(pd.read_excel('data/safely_managed_drinking.xlsx'))\n",
    "safe_manage_sanit_df = spark.createDataFrame(pd.read_excel('data/safely_managed_sanitation.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
